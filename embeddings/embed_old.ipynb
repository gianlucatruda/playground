{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Set sensible defaults\n",
    "sns.set()\n",
    "sns.set_style(\"ticks\")\n",
    "sns.set_context('talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"https://github.com/gianlucatruda/GPTools/blob/master/web2md/web2md.py\n",
    "\"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "import requests\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import feedparser\n",
    "\n",
    "\n",
    "def extract_text(url: str, ignore_images: bool) -> str:\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"headless\")\n",
    "    options.add_argument(\"disable-infobars\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--disable-software-rasterizer\")\n",
    "    options.add_argument(\"--remote-debugging-port=9222\")\n",
    "\n",
    "    # Add the following line to set a user agent\n",
    "    options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0;Win64) AppleWebkit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36\"\n",
    "    )\n",
    "    options.binary_location = \"/Applications/Google Chrome.app/Contents/MacOS/Google Chrome\"\n",
    "\n",
    "    driver = webdriver.Chrome(\n",
    "        service=ChromeService(executable_path=ChromeDriverManager().install()),\n",
    "        options=options,\n",
    "    )\n",
    "\n",
    "    driver.get(url)\n",
    "    html = driver.page_source\n",
    "    driver.quit()\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    if ignore_images:\n",
    "        for img in soup.find_all(\"img\"):\n",
    "            img.decompose()\n",
    "\n",
    "    html_content = str(soup)\n",
    "\n",
    "    h = html2text.HTML2Text()\n",
    "    h.ignore_links = False\n",
    "    h.body_width = 0\n",
    "    return h.handle(html_content)\n",
    "\n",
    "\n",
    "def get_arxiv_info(id):\n",
    "    r = requests.get(f'http://export.arxiv.org/api/query?id_list={id}')\n",
    "    feed = feedparser.parse(r.content)\n",
    "\n",
    "    title = feed.entries[0].title\n",
    "    summary = feed.entries[0].summary\n",
    "\n",
    "    return title, summary\n",
    "\n",
    "def get_github_info(user, repo):\n",
    "    r = requests.get(f'https://api.github.com/repos/{user}/{repo}')\n",
    "    data = r.json()\n",
    "\n",
    "    name = data['name']\n",
    "    description = data['description']\n",
    "\n",
    "    return name, description\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://github.com/Lightning-AI/lit-gpt Expecting value: line 1 column 1 (char 0)\n",
      "https://github.com/imartinez/privateGPT Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "url_list = [\n",
    "    \"https://github.com/invoke-ai/InvokeAI\",\n",
    "    \"https://github.com/microsoft/Data-Science-For-Beginners\",\n",
    "    \"https://github.com/chathub-dev/chathub\",\n",
    "    \"https://github.com/sweepai/sweep\",\n",
    "    \"https://github.com/karpathy/llama2.c\",\n",
    "    \"https://github.com/assafelovic/gpt-researcher\",\n",
    "    \"https://github.com/floneum/floneum\",\n",
    "    \"https://github.com/swyxio/chatgpt-mac\",\n",
    "    \"https://github.com/jamesmurdza/agenteval\",\n",
    "    \"https://github.com/langgenius/dify\",\n",
    "    \"https://github.com/1rgs/jsonformer\",\n",
    "    \"https://github.com/simonw/symbex\",\n",
    "    \"https://github.com/Lightning-AI/lit-gpt\",\n",
    "    \"https://github.com/AntonOsika/gpt-engineer\",\n",
    "    \"https://github.com/gianlucatruda/GPTools\",\n",
    "    \"https://github.com/imartinez/privateGPT\",\n",
    "    \"https://arxiv.org/abs/2307.04492\",\n",
    "    \"https://arxiv.org/abs/2307.04349\",\n",
    "    \"https://arxiv.org/abs/2307.05074\",\n",
    "    \"https://arxiv.org/abs/2307.02179\",\n",
    "    \"https://arxiv.org/abs/2307.08678\",\n",
    "    \"https://arxiv.org/abs/2307.08191\",\n",
    "    \"https://arxiv.org/abs/2307.09909\",\n",
    "    \"https://arxiv.org/abs/2307.04964\",\n",
    "    \"https://arxiv.org/abs/2307.00184\",\n",
    "    \"https://arxiv.org/abs/2307.02502\",\n",
    "    \"https://arxiv.org/abs/2306.03809\",\n",
    "    \"https://arxiv.org/abs/2306.03604\",\n",
    "    \"https://arxiv.org/abs/2305.03819\",\n",
    "    \"https://arxiv.org/abs/2303.11381\",\n",
    "    \"https://arxiv.org/abs/2306.01499\",\n",
    "    \"https://arxiv.org/abs/2306.17459\",\n",
    "    \"https://arxiv.org/abs/2307.06917\",\n",
    "\n",
    "]\n",
    "metadata = {\"url\": [], \"title\": [], \"description\": []}\n",
    "\n",
    "for url in url_list:\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        if \"github.com\" in parsed_url.netloc:\n",
    "            user, repo = parsed_url.path.strip(\"/\").split(\"/\")\n",
    "            title, description = get_github_info(user, repo)\n",
    "        elif \"arxiv.org\" in parsed_url.netloc:\n",
    "            id = parsed_url.path.split('/')[-1]  # Get the last part of the URL path\n",
    "            title, description = get_arxiv_info(id)\n",
    "        else:\n",
    "            print(f\"Unknown domain for url: {url}\")\n",
    "            continue\n",
    "\n",
    "        metadata[\"url\"].append(url)\n",
    "        metadata[\"title\"].append(title)\n",
    "        metadata[\"description\"].append(description)\n",
    "    except Exception as e:\n",
    "        print(url, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': ['https://github.com/invoke-ai/InvokeAI',\n",
       "  'https://github.com/microsoft/Data-Science-For-Beginners',\n",
       "  'https://github.com/chathub-dev/chathub',\n",
       "  'https://github.com/sweepai/sweep',\n",
       "  'https://github.com/karpathy/llama2.c',\n",
       "  'https://github.com/assafelovic/gpt-researcher',\n",
       "  'https://github.com/floneum/floneum',\n",
       "  'https://github.com/swyxio/chatgpt-mac',\n",
       "  'https://github.com/jamesmurdza/agenteval',\n",
       "  'https://github.com/langgenius/dify',\n",
       "  'https://github.com/1rgs/jsonformer',\n",
       "  'https://github.com/simonw/symbex',\n",
       "  'https://github.com/AntonOsika/gpt-engineer',\n",
       "  'https://github.com/gianlucatruda/GPTools',\n",
       "  'https://arxiv.org/abs/2307.04492',\n",
       "  'https://arxiv.org/abs/2307.04349',\n",
       "  'https://arxiv.org/abs/2307.05074',\n",
       "  'https://arxiv.org/abs/2307.02179',\n",
       "  'https://arxiv.org/abs/2307.08678',\n",
       "  'https://arxiv.org/abs/2307.08191',\n",
       "  'https://arxiv.org/abs/2307.09909',\n",
       "  'https://arxiv.org/abs/2307.04964',\n",
       "  'https://arxiv.org/abs/2307.00184',\n",
       "  'https://arxiv.org/abs/2307.02502',\n",
       "  'https://arxiv.org/abs/2306.03809',\n",
       "  'https://arxiv.org/abs/2306.03604',\n",
       "  'https://arxiv.org/abs/2305.03819',\n",
       "  'https://arxiv.org/abs/2303.11381',\n",
       "  'https://arxiv.org/abs/2306.01499',\n",
       "  'https://arxiv.org/abs/2306.17459',\n",
       "  'https://arxiv.org/abs/2307.06917'],\n",
       " 'title': ['InvokeAI',\n",
       "  'Data-Science-For-Beginners',\n",
       "  'chathub',\n",
       "  'sweep',\n",
       "  'llama2.c',\n",
       "  'gpt-researcher',\n",
       "  'floneum',\n",
       "  'chatgpt-mac',\n",
       "  'agenteval',\n",
       "  'dify',\n",
       "  'jsonformer',\n",
       "  'symbex',\n",
       "  'gpt-engineer',\n",
       "  'GPTools',\n",
       "  'Calculating Originality of LLM Assisted Source Code',\n",
       "  'RLTF: Reinforcement Learning from Unit Test Feedback',\n",
       "  'Retrieval-augmented GPT-3.5-based Text-to-SQL Framework with\\n  Sample-aware Prompting and Dynamic Revision Chain',\n",
       "  'Open-Source Large Language Models Outperform Crowd Workers and Approach\\n  ChatGPT in Text-Annotation Tasks',\n",
       "  'Do Models Explain Themselves? Counterfactual Simulatability of Natural\\n  Language Explanations',\n",
       "  'Unleashing the Potential of LLMs for Quantum Computing: A Study in\\n  Quantum Architecture Design',\n",
       "  'Chit-Chat or Deep Talk: Prompt Engineering for Process Mining',\n",
       "  'Secrets of RLHF in Large Language Models Part I: PPO',\n",
       "  'Personality Traits in Large Language Models',\n",
       "  'Math Agents: Computational Infrastructure, Mathematical Embedding, and\\n  Genomics',\n",
       "  'Can large language models democratize access to dual-use biotechnology?',\n",
       "  'Enabling Intelligent Interactions between an Agent and an LLM: A\\n  Reinforcement Learning Approach',\n",
       "  'Adapting Transformer Language Models for Predictive Typing in\\n  Brain-Computer Interfaces',\n",
       "  'MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action',\n",
       "  'Can LLMs like GPT-4 outperform traditional AI tools in dementia\\n  diagnosis? Maybe, but not today',\n",
       "  'Harnessing LLMs in Curricular Design: Using GPT-4 to Support Authoring\\n  of Learning Objectives',\n",
       "  'LLM-assisted Knowledge Graph Engineering: Experiments with ChatGPT'],\n",
       " 'description': ['InvokeAI is a leading creative engine for Stable Diffusion models, empowering professionals, artists, and enthusiasts to generate and create visual media using the latest AI-driven technologies. The solution offers an industry leading WebUI, supports terminal use through a CLI, and serves as the foundation for multiple commercial products.',\n",
       "  '10 Weeks, 20 Lessons, Data Science for All!',\n",
       "  'All-in-one chatbot client',\n",
       "  'Sweep is an AI junior developer',\n",
       "  'Inference Llama 2 in one file of pure C',\n",
       "  'GPT based autonomous agent that does online comprehensive research on any given topic',\n",
       "  'A graph editor for local AI workflows ',\n",
       "  'ChatGPT for Mac, living in your menubar.',\n",
       "  'Automated testing and benchmarking for code generation agents.',\n",
       "  'One API for plugins and datasets, one interface for prompt engineering and visual operation, all for creating powerful AI applications.',\n",
       "  'A Bulletproof Way to Generate Structured JSON from Language Models',\n",
       "  'Find the Python code for specified symbols',\n",
       "  'Specify what you want it to build, the AI asks for clarification, and then builds it.',\n",
       "  'Composable tools for doing useful things with GPT-4 (from the command line).',\n",
       "  \"The ease of using a Large Language Model (LLM) to answer a wide variety of\\nqueries and their high availability has resulted in LLMs getting integrated\\ninto various applications. LLM-based recommenders are now routinely used by\\nstudents as well as professional software programmers for code generation and\\ntesting. Though LLM-based technology has proven useful, its unethical and\\nunattributed use by students and professionals is a growing cause of concern.\\nAs such, there is a need for tools and technologies which may assist teachers\\nand other evaluators in identifying whether any portion of a source code is LLM\\ngenerated.\\n  In this paper, we propose a neural network-based tool that instructors can\\nuse to determine the original effort (and LLM's contribution) put by students\\nin writing source codes. Our tool is motivated by minimum description length\\nmeasures like Kolmogorov complexity. Our initial experiments with moderate\\nsized (up to 500 lines of code) have shown promising results that we report in\\nthis paper.\",\n",
       "  'The goal of program synthesis, or code generation, is to generate executable\\ncode based on given descriptions. Recently, there has been an increasing number\\nof studies employing reinforcement learning (RL) to improve the performance of\\nlarge language models (LLMs) for code. However, these RL methods have only used\\noffline frameworks, limiting their exploration of new sample spaces.\\nAdditionally, current approaches that utilize unit test signals are rather\\nsimple, not accounting for specific error locations within the code. To address\\nthese issues, we proposed RLTF, i.e., Reinforcement Learning from Unit Test\\nFeedback, a novel online RL framework with unit test feedback of\\nmulti-granularity for refining code LLMs. Our approach generates data in\\nreal-time during training and simultaneously utilizes fine-grained feedback\\nsignals to guide the model towards producing higher-quality code. Extensive\\nexperiments show that RLTF achieves state-of-the-art performance on the APPS\\nand the MBPP benchmarks. Our code can be found at:\\nhttps://github.com/Zyq-scut/RLTF.',\n",
       "  \"Text-to-SQL aims at generating SQL queries for the given natural language\\nquestions and thus helping users to query databases. Prompt learning with large\\nlanguage models (LLMs) has emerged as a recent approach, which designs prompts\\nto lead LLMs to understand the input question and generate the corresponding\\nSQL. However, it faces challenges with strict SQL syntax requirements. Existing\\nwork prompts the LLMs with a list of demonstration examples (i.e. question-SQL\\npairs) to generate SQL, but the fixed prompts can hardly handle the scenario\\nwhere the semantic gap between the retrieved demonstration and the input\\nquestion is large. In this paper, we propose a retrieval-augmented prompting\\nmethod for a LLM-based Text-to-SQL framework, involving sample-aware prompting\\nand a dynamic revision chain. Our approach incorporates sample-aware\\ndemonstrations, which include the composition of SQL operators and fine-grained\\ninformation related to the given question. To retrieve questions sharing\\nsimilar intents with input questions, we propose two strategies for assisting\\nretrieval. Firstly, we leverage LLMs to simplify the original questions,\\nunifying the syntax and thereby clarifying the users' intentions. To generate\\nexecutable and accurate SQLs without human intervention, we design a dynamic\\nrevision chain which iteratively adapts fine-grained feedback from the\\npreviously generated SQL. Experimental results on three Text-to-SQL benchmarks\\ndemonstrate the superiority of our method over strong baseline models.\",\n",
       "  'This study examines the performance of open-source Large Language Models\\n(LLMs) in text annotation tasks and compares it with proprietary models like\\nChatGPT and human-based services such as MTurk. While prior research\\ndemonstrated the high performance of ChatGPT across numerous NLP tasks,\\nopen-source LLMs like HugginChat and FLAN are gaining attention for their\\ncost-effectiveness, transparency, reproducibility, and superior data\\nprotection. We assess these models using both zero-shot and few-shot approaches\\nand different temperature parameters across a range of text annotation tasks.\\nOur findings show that while ChatGPT achieves the best performance in most\\ntasks, open-source LLMs not only outperform MTurk but also demonstrate\\ncompetitive potential against ChatGPT in specific tasks.',\n",
       "  'Large language models (LLMs) are trained to imitate humans to explain human\\ndecisions. However, do LLMs explain themselves? Can they help humans build\\nmental models of how LLMs process different inputs? To answer these questions,\\nwe propose to evaluate $\\\\textbf{counterfactual simulatability}$ of natural\\nlanguage explanations: whether an explanation can enable humans to precisely\\ninfer the model\\'s outputs on diverse counterfactuals of the explained input.\\nFor example, if a model answers \"yes\" to the input question \"Can eagles fly?\"\\nwith the explanation \"all birds can fly\", then humans would infer from the\\nexplanation that it would also answer \"yes\" to the counterfactual input \"Can\\npenguins fly?\". If the explanation is precise, then the model\\'s answer should\\nmatch humans\\' expectations.\\n  We implemented two metrics based on counterfactual simulatability: precision\\nand generality. We generated diverse counterfactuals automatically using LLMs.\\nWe then used these metrics to evaluate state-of-the-art LLMs (e.g., GPT-4) on\\ntwo tasks: multi-hop factual reasoning and reward modeling. We found that LLM\\'s\\nexplanations have low precision and that precision does not correlate with\\nplausibility. Therefore, naively optimizing human approvals (e.g., RLHF) may\\nnot be a sufficient solution.',\n",
       "  \"Large Language Models (LLMs) contribute significantly to the development of\\nconversational AI and has great potentials to assist the scientific research in\\nvarious areas. This paper attempts to address the following questions: What\\nopportunities do the current generation of generative pre-trained transformers\\n(GPTs) offer for the developments of noisy intermediate-scale quantum (NISQ)\\ntechnologies? Additionally, what potentials does the forthcoming generation of\\nGPTs possess to push the frontier of research in fault-tolerant quantum\\ncomputing (FTQC)? In this paper, we implement a QGAS model, which can rapidly\\npropose promising ansatz architectures and evaluate them with application\\nbenchmarks including quantum chemistry and quantum finance tasks. Our results\\ndemonstrate that after a limited number of prompt guidelines and iterations, we\\ncan obtain a high-performance ansatz which is able to produce comparable\\nresults that are achieved by state-of-the-art quantum architecture search\\nmethods. This study provides a simple overview of GPT's capabilities in\\nsupporting quantum computing research while highlighting the limitations of the\\ncurrent GPT at the same time. Additionally, we discuss futuristic applications\\nfor LLM in quantum research.\",\n",
       "  \"This research investigates the application of Large Language Models (LLMs) to\\naugment conversational agents in process mining, aiming to tackle its inherent\\ncomplexity and diverse skill requirements. While LLM advancements present novel\\nopportunities for conversational process mining, generating efficient outputs\\nis still a hurdle. We propose an innovative approach that amend many issues in\\nexisting solutions, informed by prior research on Natural Language Processing\\n(NLP) for conversational agents. Leveraging LLMs, our framework improves both\\naccessibility and agent performance, as demonstrated by experiments on public\\nquestion and data sets. Our research sets the stage for future explorations\\ninto LLMs' role in process mining and concludes with propositions for enhancing\\nLLM memory, implementing real-time user testing, and examining diverse data\\nsets.\",\n",
       "  'Large language models (LLMs) have formulated a blueprint for the advancement\\nof artificial general intelligence. Its primary objective is to function as a\\nhuman-centric (helpful, honest, and harmless) assistant. Alignment with humans\\nassumes paramount significance, and reinforcement learning with human feedback\\n(RLHF) emerges as the pivotal technological paradigm underpinning this pursuit.\\nCurrent technical routes usually include \\\\textbf{reward models} to measure\\nhuman preferences, \\\\textbf{Proximal Policy Optimization} (PPO) to optimize\\npolicy model outputs, and \\\\textbf{process supervision} to improve step-by-step\\nreasoning capabilities. However, due to the challenges of reward design,\\nenvironment interaction, and agent training, coupled with huge trial and error\\ncost of large language models, there is a significant barrier for AI\\nresearchers to motivate the development of technical alignment and safe landing\\nof LLMs. The stable training of RLHF has still been a puzzle. In the first\\nreport, we dissect the framework of RLHF, re-evaluate the inner workings of\\nPPO, and explore how the parts comprising PPO algorithms impact policy agent\\ntraining. We identify policy constraints being the key factor for the effective\\nimplementation of the PPO algorithm. Therefore, we explore the PPO-max, an\\nadvanced version of PPO algorithm, to efficiently improve the training\\nstability of the policy model. Based on our main results, we perform a\\ncomprehensive analysis of RLHF abilities compared with SFT models and ChatGPT.\\nThe absence of open-source implementations has posed significant challenges to\\nthe investigation of LLMs alignment. Therefore, we are eager to release\\ntechnical reports, reward models and PPO codes, aiming to make modest\\ncontributions to the advancement of LLMs.',\n",
       "  'The advent of large language models (LLMs) has revolutionized natural\\nlanguage processing, enabling the generation of coherent and contextually\\nrelevant text. As LLMs increasingly power conversational agents, the\\nsynthesized personality embedded in these models by virtue of their training on\\nlarge amounts of human-generated data draws attention. Since personality is an\\nimportant factor determining the effectiveness of communication, we present a\\ncomprehensive method for administering validated psychometric tests and\\nquantifying, analyzing, and shaping personality traits exhibited in text\\ngenerated from widely-used LLMs. We find that: 1) personality simulated in the\\noutputs of some LLMs (under specific prompting configurations) is reliable and\\nvalid; 2) evidence of reliability and validity of LLM-simulated personality is\\nstronger for larger and instruction fine-tuned models; and 3) personality in\\nLLM outputs can be shaped along desired dimensions to mimic specific\\npersonality profiles. We also discuss potential applications and ethical\\nimplications of our measurement and shaping framework, especially regarding\\nresponsible use of LLMs.',\n",
       "  'The advancement in generative AI could be boosted with more accessible\\nmathematics. Beyond human-AI chat, large language models (LLMs) are emerging in\\nprogramming, algorithm discovery, and theorem proving, yet their genomics\\napplication is limited. This project introduces Math Agents and mathematical\\nembedding as fresh entries to the \"Moore\\'s Law of Mathematics\", using a\\nGPT-based workflow to convert equations from literature into LaTeX and Python\\nformats. While many digital equation representations exist, there\\'s a lack of\\nautomated large-scale evaluation tools. LLMs are pivotal as linguistic user\\ninterfaces, providing natural language access for human-AI chat and formal\\nlanguages for large-scale AI-assisted computational infrastructure. Given the\\ninfinite formal possibility spaces, Math Agents, which interact with math,\\ncould potentially shift us from \"big data\" to \"big math\". Math, unlike the more\\nflexible natural language, has properties subject to proof, enabling its use\\nbeyond traditional applications like high-validation math-certified icons for\\nAI alignment aims. This project aims to use Math Agents and mathematical\\nembeddings to address the ageing issue in information systems biology by\\napplying multiscalar physics mathematics to disease models and genomic data.\\nGenerative AI with episodic memory could help analyse causal relations in\\nlongitudinal health records, using SIR Precision Health models. Genomic data is\\nsuggested for addressing the unsolved Alzheimer\\'s disease problem.',\n",
       "  \"Large language models (LLMs) such as those embedded in 'chatbots' are\\naccelerating and democratizing research by providing comprehensible information\\nand expertise from many different fields. However, these models may also confer\\neasy access to dual-use technologies capable of inflicting great harm. To\\nevaluate this risk, the 'Safeguarding the Future' course at MIT tasked\\nnon-scientist students with investigating whether LLM chatbots could be\\nprompted to assist non-experts in causing a pandemic. In one hour, the chatbots\\nsuggested four potential pandemic pathogens, explained how they can be\\ngenerated from synthetic DNA using reverse genetics, supplied the names of DNA\\nsynthesis companies unlikely to screen orders, identified detailed protocols\\nand how to troubleshoot them, and recommended that anyone lacking the skills to\\nperform reverse genetics engage a core facility or contract research\\norganization. Collectively, these results suggest that LLMs will make\\npandemic-class agents widely accessible as soon as they are credibly\\nidentified, even to people with little or no laboratory training. Promising\\nnonproliferation measures include pre-release evaluations of LLMs by third\\nparties, curating training datasets to remove harmful concepts, and verifiably\\nscreening all DNA generated by synthesis providers or used by contract research\\norganizations and robotic cloud laboratories to engineer organisms or viruses.\",\n",
       "  \"Large language models (LLMs) encode a vast amount of world knowledge acquired\\nfrom massive text datasets. Recent studies have demonstrated that LLMs can\\nassist an agent in solving complex sequential decision making tasks in embodied\\nenvironments by providing high-level instructions. However, interacting with\\nLLMs can be time-consuming, as in many practical scenarios, they require a\\nsignificant amount of storage space that can only be deployed on remote cloud\\nserver nodes. Additionally, using commercial LLMs can be costly since they may\\ncharge based on usage frequency. In this paper, we explore how to enable\\nintelligent cost-effective interactions between the agent and an LLM. We\\npropose a reinforcement learning based mediator model that determines when it\\nis necessary to consult LLMs for high-level instructions to accomplish a target\\ntask. Experiments on 4 MiniGrid environments that entail planning sub-goals\\ndemonstrate that our method can learn to solve target tasks with only a few\\nnecessary interactions with an LLM, significantly reducing interaction costs in\\ntesting environments, compared with baseline methods. Experimental results also\\nsuggest that by learning a mediator model to interact with the LLM, the agent's\\nperformance becomes more robust against partial observability of the\\nenvironment. Our code is available at https://github.com/ZJLAB-AMMI/LLM4RL.\",\n",
       "  'Brain-computer interfaces (BCI) are an important mode of alternative and\\naugmentative communication for many people. Unlike keyboards, many BCI systems\\ndo not display even the 26 letters of English at one time, let alone all the\\nsymbols in more complex systems. Using language models to make character-level\\npredictions, therefore, can greatly speed up BCI typing (Ghosh and Kristensson,\\n2017). While most existing BCI systems employ character n-gram models or no LM\\nat all, this paper adapts several wordpiece-level Transformer LMs to make\\ncharacter predictions and evaluates them on typing tasks. GPT-2 fares best on\\nclean text, but different LMs react differently to noisy histories. We further\\nanalyze the effect of character positions in a word and context lengths.',\n",
       "  \"We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of\\nvision experts to achieve multimodal reasoning and action. In this paper, we\\ndefine and explore a comprehensive list of advanced vision tasks that are\\nintriguing to solve, but may exceed the capabilities of existing vision and\\nvision-language models. To achieve such advanced visual intelligence, MM-REACT\\nintroduces a textual prompt design that can represent text descriptions,\\ntextualized spatial coordinates, and aligned file names for dense visual\\nsignals such as images and videos. MM-REACT's prompt design allows language\\nmodels to accept, associate, and process multimodal information, thereby\\nfacilitating the synergetic combination of ChatGPT and various vision experts.\\nZero-shot experiments demonstrate MM-REACT's effectiveness in addressing the\\nspecified capabilities of interests and its wide application in different\\nscenarios that require advanced visual understanding. Furthermore, we discuss\\nand compare MM-REACT's system paradigm with an alternative approach that\\nextends language models for multimodal scenarios through joint finetuning.\\nCode, demo, video, and visualization are available at\\nhttps://multimodal-react.github.io/\",\n",
       "  'Recent investigations show that large language models (LLMs), specifically\\nGPT-4, not only have remarkable capabilities in common Natural Language\\nProcessing (NLP) tasks but also exhibit human-level performance on various\\nprofessional and academic benchmarks. However, whether GPT-4 can be directly\\nused in practical applications and replace traditional artificial intelligence\\n(AI) tools in specialized domains requires further experimental validation. In\\nthis paper, we explore the potential of LLMs such as GPT-4 to outperform\\ntraditional AI tools in dementia diagnosis. Comprehensive comparisons between\\nGPT-4 and traditional AI tools are conducted to examine their diagnostic\\naccuracy in a clinical setting. Experimental results on two real clinical\\ndatasets show that, although LLMs like GPT-4 demonstrate potential for future\\nadvancements in dementia diagnosis, they currently do not surpass the\\nperformance of traditional AI tools. The interpretability and faithfulness of\\nGPT-4 are also evaluated by comparison with real doctors. We discuss the\\nlimitations of GPT-4 in its current state and propose future research\\ndirections to enhance GPT-4 in dementia diagnosis.',\n",
       "  \"We evaluated the capability of a generative pre-trained transformer (GPT-4)\\nto automatically generate high-quality learning objectives (LOs) in the context\\nof a practically oriented university course on Artificial Intelligence.\\nDiscussions of opportunities (e.g., content generation, explanation) and risks\\n(e.g., cheating) of this emerging technology in education have intensified, but\\nto date there has not been a study of the models' capabilities in supporting\\nthe course design and authoring of LOs. LOs articulate the knowledge and skills\\nlearners are intended to acquire by engaging with a course. To be effective,\\nLOs must focus on what students are intended to achieve, focus on specific\\ncognitive processes, and be measurable. Thus, authoring high-quality LOs is a\\nchallenging and time consuming (i.e., expensive) effort. We evaluated 127 LOs\\nthat were automatically generated based on a carefully crafted prompt (detailed\\nguidelines on high-quality LOs authoring) submitted to GPT-4 for conceptual\\nmodules and projects of an AI Practitioner course. We analyzed the generated\\nLOs if they follow certain best practices such as beginning with action verbs\\nfrom Bloom's taxonomy in regards to the level of sophistication intended. Our\\nanalysis showed that the generated LOs are sensible, properly expressed (e.g.,\\nstarting with an action verb), and that they largely operate at the appropriate\\nlevel of Bloom's taxonomy, respecting the different nature of the conceptual\\nmodules (lower levels) and projects (higher levels). Our results can be\\nleveraged by instructors and curricular designers wishing to take advantage of\\nthe state-of-the-art generative models to support their curricular and course\\ndesign efforts.\",\n",
       "  'Knowledge Graphs (KG) provide us with a structured, flexible, transparent,\\ncross-system, and collaborative way of organizing our knowledge and data across\\nvarious domains in society and industrial as well as scientific disciplines.\\nKGs surpass any other form of representation in terms of effectiveness.\\nHowever, Knowledge Graph Engineering (KGE) requires in-depth experiences of\\ngraph structures, web technologies, existing models and vocabularies, rule\\nsets, logic, as well as best practices. It also demands a significant amount of\\nwork. Considering the advancements in large language models (LLMs) and their\\ninterfaces and applications in recent years, we have conducted comprehensive\\nexperiments with ChatGPT to explore its potential in supporting KGE. In this\\npaper, we present a selection of these experiments and their results to\\ndemonstrate how ChatGPT can assist us in the development and management of KGs.']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/invoke-ai/InvokeAI</td>\n",
       "      <td>InvokeAI</td>\n",
       "      <td>InvokeAI is a leading creative engine for Stab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/microsoft/Data-Science-For-...</td>\n",
       "      <td>Data-Science-For-Beginners</td>\n",
       "      <td>10 Weeks, 20 Lessons, Data Science for All!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/chathub-dev/chathub</td>\n",
       "      <td>chathub</td>\n",
       "      <td>All-in-one chatbot client</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/sweepai/sweep</td>\n",
       "      <td>sweep</td>\n",
       "      <td>Sweep is an AI junior developer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://github.com/karpathy/llama2.c</td>\n",
       "      <td>llama2.c</td>\n",
       "      <td>Inference Llama 2 in one file of pure C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://github.com/assafelovic/gpt-researcher</td>\n",
       "      <td>gpt-researcher</td>\n",
       "      <td>GPT based autonomous agent that does online co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://github.com/floneum/floneum</td>\n",
       "      <td>floneum</td>\n",
       "      <td>A graph editor for local AI workflows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://github.com/swyxio/chatgpt-mac</td>\n",
       "      <td>chatgpt-mac</td>\n",
       "      <td>ChatGPT for Mac, living in your menubar.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://github.com/jamesmurdza/agenteval</td>\n",
       "      <td>agenteval</td>\n",
       "      <td>Automated testing and benchmarking for code ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://github.com/langgenius/dify</td>\n",
       "      <td>dify</td>\n",
       "      <td>One API for plugins and datasets, one interfac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://github.com/1rgs/jsonformer</td>\n",
       "      <td>jsonformer</td>\n",
       "      <td>A Bulletproof Way to Generate Structured JSON ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://github.com/simonw/symbex</td>\n",
       "      <td>symbex</td>\n",
       "      <td>Find the Python code for specified symbols</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://github.com/AntonOsika/gpt-engineer</td>\n",
       "      <td>gpt-engineer</td>\n",
       "      <td>Specify what you want it to build, the AI asks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://github.com/gianlucatruda/GPTools</td>\n",
       "      <td>GPTools</td>\n",
       "      <td>Composable tools for doing useful things with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://arxiv.org/abs/2307.04492</td>\n",
       "      <td>Calculating Originality of LLM Assisted Source...</td>\n",
       "      <td>The ease of using a Large Language Model (LLM)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://arxiv.org/abs/2307.04349</td>\n",
       "      <td>RLTF: Reinforcement Learning from Unit Test Fe...</td>\n",
       "      <td>The goal of program synthesis, or code generat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://arxiv.org/abs/2307.05074</td>\n",
       "      <td>Retrieval-augmented GPT-3.5-based Text-to-SQL ...</td>\n",
       "      <td>Text-to-SQL aims at generating SQL queries for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://arxiv.org/abs/2307.02179</td>\n",
       "      <td>Open-Source Large Language Models Outperform C...</td>\n",
       "      <td>This study examines the performance of open-so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://arxiv.org/abs/2307.08678</td>\n",
       "      <td>Do Models Explain Themselves? Counterfactual S...</td>\n",
       "      <td>Large language models (LLMs) are trained to im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://arxiv.org/abs/2307.08191</td>\n",
       "      <td>Unleashing the Potential of LLMs for Quantum C...</td>\n",
       "      <td>Large Language Models (LLMs) contribute signif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>https://arxiv.org/abs/2307.09909</td>\n",
       "      <td>Chit-Chat or Deep Talk: Prompt Engineering for...</td>\n",
       "      <td>This research investigates the application of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>https://arxiv.org/abs/2307.04964</td>\n",
       "      <td>Secrets of RLHF in Large Language Models Part ...</td>\n",
       "      <td>Large language models (LLMs) have formulated a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>https://arxiv.org/abs/2307.00184</td>\n",
       "      <td>Personality Traits in Large Language Models</td>\n",
       "      <td>The advent of large language models (LLMs) has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>https://arxiv.org/abs/2307.02502</td>\n",
       "      <td>Math Agents: Computational Infrastructure, Mat...</td>\n",
       "      <td>The advancement in generative AI could be boos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>https://arxiv.org/abs/2306.03809</td>\n",
       "      <td>Can large language models democratize access t...</td>\n",
       "      <td>Large language models (LLMs) such as those emb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>https://arxiv.org/abs/2306.03604</td>\n",
       "      <td>Enabling Intelligent Interactions between an A...</td>\n",
       "      <td>Large language models (LLMs) encode a vast amo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>https://arxiv.org/abs/2305.03819</td>\n",
       "      <td>Adapting Transformer Language Models for Predi...</td>\n",
       "      <td>Brain-computer interfaces (BCI) are an importa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>https://arxiv.org/abs/2303.11381</td>\n",
       "      <td>MM-REACT: Prompting ChatGPT for Multimodal Rea...</td>\n",
       "      <td>We propose MM-REACT, a system paradigm that in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>https://arxiv.org/abs/2306.01499</td>\n",
       "      <td>Can LLMs like GPT-4 outperform traditional AI ...</td>\n",
       "      <td>Recent investigations show that large language...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>https://arxiv.org/abs/2306.17459</td>\n",
       "      <td>Harnessing LLMs in Curricular Design: Using GP...</td>\n",
       "      <td>We evaluated the capability of a generative pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>https://arxiv.org/abs/2307.06917</td>\n",
       "      <td>LLM-assisted Knowledge Graph Engineering: Expe...</td>\n",
       "      <td>Knowledge Graphs (KG) provide us with a struct...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  url  \\\n",
       "0               https://github.com/invoke-ai/InvokeAI   \n",
       "1   https://github.com/microsoft/Data-Science-For-...   \n",
       "2              https://github.com/chathub-dev/chathub   \n",
       "3                    https://github.com/sweepai/sweep   \n",
       "4                https://github.com/karpathy/llama2.c   \n",
       "5       https://github.com/assafelovic/gpt-researcher   \n",
       "6                  https://github.com/floneum/floneum   \n",
       "7               https://github.com/swyxio/chatgpt-mac   \n",
       "8            https://github.com/jamesmurdza/agenteval   \n",
       "9                  https://github.com/langgenius/dify   \n",
       "10                 https://github.com/1rgs/jsonformer   \n",
       "11                   https://github.com/simonw/symbex   \n",
       "12         https://github.com/AntonOsika/gpt-engineer   \n",
       "13           https://github.com/gianlucatruda/GPTools   \n",
       "14                   https://arxiv.org/abs/2307.04492   \n",
       "15                   https://arxiv.org/abs/2307.04349   \n",
       "16                   https://arxiv.org/abs/2307.05074   \n",
       "17                   https://arxiv.org/abs/2307.02179   \n",
       "18                   https://arxiv.org/abs/2307.08678   \n",
       "19                   https://arxiv.org/abs/2307.08191   \n",
       "20                   https://arxiv.org/abs/2307.09909   \n",
       "21                   https://arxiv.org/abs/2307.04964   \n",
       "22                   https://arxiv.org/abs/2307.00184   \n",
       "23                   https://arxiv.org/abs/2307.02502   \n",
       "24                   https://arxiv.org/abs/2306.03809   \n",
       "25                   https://arxiv.org/abs/2306.03604   \n",
       "26                   https://arxiv.org/abs/2305.03819   \n",
       "27                   https://arxiv.org/abs/2303.11381   \n",
       "28                   https://arxiv.org/abs/2306.01499   \n",
       "29                   https://arxiv.org/abs/2306.17459   \n",
       "30                   https://arxiv.org/abs/2307.06917   \n",
       "\n",
       "                                                title  \\\n",
       "0                                            InvokeAI   \n",
       "1                          Data-Science-For-Beginners   \n",
       "2                                             chathub   \n",
       "3                                               sweep   \n",
       "4                                            llama2.c   \n",
       "5                                      gpt-researcher   \n",
       "6                                             floneum   \n",
       "7                                         chatgpt-mac   \n",
       "8                                           agenteval   \n",
       "9                                                dify   \n",
       "10                                         jsonformer   \n",
       "11                                             symbex   \n",
       "12                                       gpt-engineer   \n",
       "13                                            GPTools   \n",
       "14  Calculating Originality of LLM Assisted Source...   \n",
       "15  RLTF: Reinforcement Learning from Unit Test Fe...   \n",
       "16  Retrieval-augmented GPT-3.5-based Text-to-SQL ...   \n",
       "17  Open-Source Large Language Models Outperform C...   \n",
       "18  Do Models Explain Themselves? Counterfactual S...   \n",
       "19  Unleashing the Potential of LLMs for Quantum C...   \n",
       "20  Chit-Chat or Deep Talk: Prompt Engineering for...   \n",
       "21  Secrets of RLHF in Large Language Models Part ...   \n",
       "22        Personality Traits in Large Language Models   \n",
       "23  Math Agents: Computational Infrastructure, Mat...   \n",
       "24  Can large language models democratize access t...   \n",
       "25  Enabling Intelligent Interactions between an A...   \n",
       "26  Adapting Transformer Language Models for Predi...   \n",
       "27  MM-REACT: Prompting ChatGPT for Multimodal Rea...   \n",
       "28  Can LLMs like GPT-4 outperform traditional AI ...   \n",
       "29  Harnessing LLMs in Curricular Design: Using GP...   \n",
       "30  LLM-assisted Knowledge Graph Engineering: Expe...   \n",
       "\n",
       "                                          description  \n",
       "0   InvokeAI is a leading creative engine for Stab...  \n",
       "1         10 Weeks, 20 Lessons, Data Science for All!  \n",
       "2                           All-in-one chatbot client  \n",
       "3                     Sweep is an AI junior developer  \n",
       "4             Inference Llama 2 in one file of pure C  \n",
       "5   GPT based autonomous agent that does online co...  \n",
       "6              A graph editor for local AI workflows   \n",
       "7            ChatGPT for Mac, living in your menubar.  \n",
       "8   Automated testing and benchmarking for code ge...  \n",
       "9   One API for plugins and datasets, one interfac...  \n",
       "10  A Bulletproof Way to Generate Structured JSON ...  \n",
       "11         Find the Python code for specified symbols  \n",
       "12  Specify what you want it to build, the AI asks...  \n",
       "13  Composable tools for doing useful things with ...  \n",
       "14  The ease of using a Large Language Model (LLM)...  \n",
       "15  The goal of program synthesis, or code generat...  \n",
       "16  Text-to-SQL aims at generating SQL queries for...  \n",
       "17  This study examines the performance of open-so...  \n",
       "18  Large language models (LLMs) are trained to im...  \n",
       "19  Large Language Models (LLMs) contribute signif...  \n",
       "20  This research investigates the application of ...  \n",
       "21  Large language models (LLMs) have formulated a...  \n",
       "22  The advent of large language models (LLMs) has...  \n",
       "23  The advancement in generative AI could be boos...  \n",
       "24  Large language models (LLMs) such as those emb...  \n",
       "25  Large language models (LLMs) encode a vast amo...  \n",
       "26  Brain-computer interfaces (BCI) are an importa...  \n",
       "27  We propose MM-REACT, a system paradigm that in...  \n",
       "28  Recent investigations show that large language...  \n",
       "29  We evaluated the capability of a generative pr...  \n",
       "30  Knowledge Graphs (KG) provide us with a struct...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metadata = pd.DataFrame(metadata)\n",
    "df_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata.to_csv(\"metadata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31 entries, 0 to 30\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   url          31 non-null     object\n",
      " 1   title        31 non-null     object\n",
      " 2   description  31 non-null     object\n",
      " 3   embed_index  31 non-null     int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 1.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_metadata['embed_index'] = -1\n",
    "df_metadata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://github.com/invoke-ai/InvokeAI InvokeAI\n",
      "https://github.com/microsoft/Data-Science-For-Beginners Data-Science-For-Beginners\n",
      "https://github.com/chathub-dev/chathub chathub\n",
      "https://github.com/sweepai/sweep sweep\n",
      "https://github.com/karpathy/llama2.c llama2.c\n",
      "https://github.com/assafelovic/gpt-researcher gpt-researcher\n",
      "https://github.com/floneum/floneum floneum\n",
      "https://github.com/swyxio/chatgpt-mac chatgpt-mac\n",
      "https://github.com/jamesmurdza/agenteval agenteval\n",
      "https://github.com/langgenius/dify dify\n",
      "https://github.com/1rgs/jsonformer jsonformer\n",
      "https://github.com/simonw/symbex symbex\n",
      "https://github.com/AntonOsika/gpt-engineer gpt-engineer\n",
      "https://github.com/gianlucatruda/GPTools GPTools\n",
      "https://arxiv.org/abs/2307.04492 Calculating Originality of LLM Assisted Source Code\n",
      "https://arxiv.org/abs/2307.04349 RLTF: Reinforcement Learning from Unit Test Feedback\n",
      "https://arxiv.org/abs/2307.05074 Retrieval-augmented GPT-3.5-based Text-to-SQL Framework with\n",
      "  Sample-aware Prompting and Dynamic Revision Chain\n",
      "https://arxiv.org/abs/2307.02179 Open-Source Large Language Models Outperform Crowd Workers and Approach\n",
      "  ChatGPT in Text-Annotation Tasks\n",
      "https://arxiv.org/abs/2307.08678 Do Models Explain Themselves? Counterfactual Simulatability of Natural\n",
      "  Language Explanations\n",
      "https://arxiv.org/abs/2307.08191 Unleashing the Potential of LLMs for Quantum Computing: A Study in\n",
      "  Quantum Architecture Design\n",
      "https://arxiv.org/abs/2307.09909 Chit-Chat or Deep Talk: Prompt Engineering for Process Mining\n",
      "https://arxiv.org/abs/2307.04964 Secrets of RLHF in Large Language Models Part I: PPO\n",
      "https://arxiv.org/abs/2307.00184 Personality Traits in Large Language Models\n",
      "https://arxiv.org/abs/2307.02502 Math Agents: Computational Infrastructure, Mathematical Embedding, and\n",
      "  Genomics\n",
      "https://arxiv.org/abs/2306.03809 Can large language models democratize access to dual-use biotechnology?\n",
      "https://arxiv.org/abs/2306.03604 Enabling Intelligent Interactions between an Agent and an LLM: A\n",
      "  Reinforcement Learning Approach\n",
      "https://arxiv.org/abs/2305.03819 Adapting Transformer Language Models for Predictive Typing in\n",
      "  Brain-Computer Interfaces\n",
      "https://arxiv.org/abs/2303.11381 MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action\n",
      "https://arxiv.org/abs/2306.01499 Can LLMs like GPT-4 outperform traditional AI tools in dementia\n",
      "  diagnosis? Maybe, but not today\n",
      "https://arxiv.org/abs/2306.17459 Harnessing LLMs in Curricular Design: Using GPT-4 to Support Authoring\n",
      "  of Learning Objectives\n",
      "https://arxiv.org/abs/2307.06917 LLM-assisted Knowledge Graph Engineering: Experiments with ChatGPT\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import dotenv\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def get_embeddings(text):\n",
    "    response = openai.Embedding.create(\n",
    "      model=\"text-embedding-ada-002\",\n",
    "      input=text,\n",
    "    )\n",
    "    return response['data'][0]['embedding']\n",
    "\n",
    "embeddings = []\n",
    "for i, row in df_metadata.iterrows():\n",
    "    url, title, description = row['url'], row['title'], row['description']\n",
    "    print(url, title)\n",
    "    \n",
    "    try:\n",
    "        # Combine the title and description for embedding\n",
    "        combined_text = title + \" \" + description\n",
    "        embedding = get_embeddings(combined_text)\n",
    "        embeddings.append(embedding)\n",
    "        df_metadata.loc[i, \"embed_index\"] = i\n",
    "    except Exception as e:\n",
    "        print(url, e)\n",
    "        # raise e\n",
    "\n",
    "# Convert embeddings list to numpy array\n",
    "embeddings_array = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 1536)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>embed_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/invoke-ai/InvokeAI</td>\n",
       "      <td>InvokeAI</td>\n",
       "      <td>InvokeAI is a leading creative engine for Stab...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/microsoft/Data-Science-For-...</td>\n",
       "      <td>Data-Science-For-Beginners</td>\n",
       "      <td>10 Weeks, 20 Lessons, Data Science for All!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/chathub-dev/chathub</td>\n",
       "      <td>chathub</td>\n",
       "      <td>All-in-one chatbot client</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/sweepai/sweep</td>\n",
       "      <td>sweep</td>\n",
       "      <td>Sweep is an AI junior developer</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://github.com/karpathy/llama2.c</td>\n",
       "      <td>llama2.c</td>\n",
       "      <td>Inference Llama 2 in one file of pure C</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://github.com/assafelovic/gpt-researcher</td>\n",
       "      <td>gpt-researcher</td>\n",
       "      <td>GPT based autonomous agent that does online co...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://github.com/floneum/floneum</td>\n",
       "      <td>floneum</td>\n",
       "      <td>A graph editor for local AI workflows</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://github.com/swyxio/chatgpt-mac</td>\n",
       "      <td>chatgpt-mac</td>\n",
       "      <td>ChatGPT for Mac, living in your menubar.</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://github.com/jamesmurdza/agenteval</td>\n",
       "      <td>agenteval</td>\n",
       "      <td>Automated testing and benchmarking for code ge...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://github.com/langgenius/dify</td>\n",
       "      <td>dify</td>\n",
       "      <td>One API for plugins and datasets, one interfac...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://github.com/1rgs/jsonformer</td>\n",
       "      <td>jsonformer</td>\n",
       "      <td>A Bulletproof Way to Generate Structured JSON ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://github.com/simonw/symbex</td>\n",
       "      <td>symbex</td>\n",
       "      <td>Find the Python code for specified symbols</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://github.com/AntonOsika/gpt-engineer</td>\n",
       "      <td>gpt-engineer</td>\n",
       "      <td>Specify what you want it to build, the AI asks...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://github.com/gianlucatruda/GPTools</td>\n",
       "      <td>GPTools</td>\n",
       "      <td>Composable tools for doing useful things with ...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://arxiv.org/abs/2307.04492</td>\n",
       "      <td>Calculating Originality of LLM Assisted Source...</td>\n",
       "      <td>The ease of using a Large Language Model (LLM)...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://arxiv.org/abs/2307.04349</td>\n",
       "      <td>RLTF: Reinforcement Learning from Unit Test Fe...</td>\n",
       "      <td>The goal of program synthesis, or code generat...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://arxiv.org/abs/2307.05074</td>\n",
       "      <td>Retrieval-augmented GPT-3.5-based Text-to-SQL ...</td>\n",
       "      <td>Text-to-SQL aims at generating SQL queries for...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://arxiv.org/abs/2307.02179</td>\n",
       "      <td>Open-Source Large Language Models Outperform C...</td>\n",
       "      <td>This study examines the performance of open-so...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://arxiv.org/abs/2307.08678</td>\n",
       "      <td>Do Models Explain Themselves? Counterfactual S...</td>\n",
       "      <td>Large language models (LLMs) are trained to im...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://arxiv.org/abs/2307.08191</td>\n",
       "      <td>Unleashing the Potential of LLMs for Quantum C...</td>\n",
       "      <td>Large Language Models (LLMs) contribute signif...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>https://arxiv.org/abs/2307.09909</td>\n",
       "      <td>Chit-Chat or Deep Talk: Prompt Engineering for...</td>\n",
       "      <td>This research investigates the application of ...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>https://arxiv.org/abs/2307.04964</td>\n",
       "      <td>Secrets of RLHF in Large Language Models Part ...</td>\n",
       "      <td>Large language models (LLMs) have formulated a...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>https://arxiv.org/abs/2307.00184</td>\n",
       "      <td>Personality Traits in Large Language Models</td>\n",
       "      <td>The advent of large language models (LLMs) has...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>https://arxiv.org/abs/2307.02502</td>\n",
       "      <td>Math Agents: Computational Infrastructure, Mat...</td>\n",
       "      <td>The advancement in generative AI could be boos...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>https://arxiv.org/abs/2306.03809</td>\n",
       "      <td>Can large language models democratize access t...</td>\n",
       "      <td>Large language models (LLMs) such as those emb...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>https://arxiv.org/abs/2306.03604</td>\n",
       "      <td>Enabling Intelligent Interactions between an A...</td>\n",
       "      <td>Large language models (LLMs) encode a vast amo...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>https://arxiv.org/abs/2305.03819</td>\n",
       "      <td>Adapting Transformer Language Models for Predi...</td>\n",
       "      <td>Brain-computer interfaces (BCI) are an importa...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>https://arxiv.org/abs/2303.11381</td>\n",
       "      <td>MM-REACT: Prompting ChatGPT for Multimodal Rea...</td>\n",
       "      <td>We propose MM-REACT, a system paradigm that in...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>https://arxiv.org/abs/2306.01499</td>\n",
       "      <td>Can LLMs like GPT-4 outperform traditional AI ...</td>\n",
       "      <td>Recent investigations show that large language...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>https://arxiv.org/abs/2306.17459</td>\n",
       "      <td>Harnessing LLMs in Curricular Design: Using GP...</td>\n",
       "      <td>We evaluated the capability of a generative pr...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>https://arxiv.org/abs/2307.06917</td>\n",
       "      <td>LLM-assisted Knowledge Graph Engineering: Expe...</td>\n",
       "      <td>Knowledge Graphs (KG) provide us with a struct...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  url  \\\n",
       "0               https://github.com/invoke-ai/InvokeAI   \n",
       "1   https://github.com/microsoft/Data-Science-For-...   \n",
       "2              https://github.com/chathub-dev/chathub   \n",
       "3                    https://github.com/sweepai/sweep   \n",
       "4                https://github.com/karpathy/llama2.c   \n",
       "5       https://github.com/assafelovic/gpt-researcher   \n",
       "6                  https://github.com/floneum/floneum   \n",
       "7               https://github.com/swyxio/chatgpt-mac   \n",
       "8            https://github.com/jamesmurdza/agenteval   \n",
       "9                  https://github.com/langgenius/dify   \n",
       "10                 https://github.com/1rgs/jsonformer   \n",
       "11                   https://github.com/simonw/symbex   \n",
       "12         https://github.com/AntonOsika/gpt-engineer   \n",
       "13           https://github.com/gianlucatruda/GPTools   \n",
       "14                   https://arxiv.org/abs/2307.04492   \n",
       "15                   https://arxiv.org/abs/2307.04349   \n",
       "16                   https://arxiv.org/abs/2307.05074   \n",
       "17                   https://arxiv.org/abs/2307.02179   \n",
       "18                   https://arxiv.org/abs/2307.08678   \n",
       "19                   https://arxiv.org/abs/2307.08191   \n",
       "20                   https://arxiv.org/abs/2307.09909   \n",
       "21                   https://arxiv.org/abs/2307.04964   \n",
       "22                   https://arxiv.org/abs/2307.00184   \n",
       "23                   https://arxiv.org/abs/2307.02502   \n",
       "24                   https://arxiv.org/abs/2306.03809   \n",
       "25                   https://arxiv.org/abs/2306.03604   \n",
       "26                   https://arxiv.org/abs/2305.03819   \n",
       "27                   https://arxiv.org/abs/2303.11381   \n",
       "28                   https://arxiv.org/abs/2306.01499   \n",
       "29                   https://arxiv.org/abs/2306.17459   \n",
       "30                   https://arxiv.org/abs/2307.06917   \n",
       "\n",
       "                                                title  \\\n",
       "0                                            InvokeAI   \n",
       "1                          Data-Science-For-Beginners   \n",
       "2                                             chathub   \n",
       "3                                               sweep   \n",
       "4                                            llama2.c   \n",
       "5                                      gpt-researcher   \n",
       "6                                             floneum   \n",
       "7                                         chatgpt-mac   \n",
       "8                                           agenteval   \n",
       "9                                                dify   \n",
       "10                                         jsonformer   \n",
       "11                                             symbex   \n",
       "12                                       gpt-engineer   \n",
       "13                                            GPTools   \n",
       "14  Calculating Originality of LLM Assisted Source...   \n",
       "15  RLTF: Reinforcement Learning from Unit Test Fe...   \n",
       "16  Retrieval-augmented GPT-3.5-based Text-to-SQL ...   \n",
       "17  Open-Source Large Language Models Outperform C...   \n",
       "18  Do Models Explain Themselves? Counterfactual S...   \n",
       "19  Unleashing the Potential of LLMs for Quantum C...   \n",
       "20  Chit-Chat or Deep Talk: Prompt Engineering for...   \n",
       "21  Secrets of RLHF in Large Language Models Part ...   \n",
       "22        Personality Traits in Large Language Models   \n",
       "23  Math Agents: Computational Infrastructure, Mat...   \n",
       "24  Can large language models democratize access t...   \n",
       "25  Enabling Intelligent Interactions between an A...   \n",
       "26  Adapting Transformer Language Models for Predi...   \n",
       "27  MM-REACT: Prompting ChatGPT for Multimodal Rea...   \n",
       "28  Can LLMs like GPT-4 outperform traditional AI ...   \n",
       "29  Harnessing LLMs in Curricular Design: Using GP...   \n",
       "30  LLM-assisted Knowledge Graph Engineering: Expe...   \n",
       "\n",
       "                                          description  embed_index  \n",
       "0   InvokeAI is a leading creative engine for Stab...            0  \n",
       "1         10 Weeks, 20 Lessons, Data Science for All!            1  \n",
       "2                           All-in-one chatbot client            2  \n",
       "3                     Sweep is an AI junior developer            3  \n",
       "4             Inference Llama 2 in one file of pure C            4  \n",
       "5   GPT based autonomous agent that does online co...            5  \n",
       "6              A graph editor for local AI workflows             6  \n",
       "7            ChatGPT for Mac, living in your menubar.            7  \n",
       "8   Automated testing and benchmarking for code ge...            8  \n",
       "9   One API for plugins and datasets, one interfac...            9  \n",
       "10  A Bulletproof Way to Generate Structured JSON ...           10  \n",
       "11         Find the Python code for specified symbols           11  \n",
       "12  Specify what you want it to build, the AI asks...           12  \n",
       "13  Composable tools for doing useful things with ...           13  \n",
       "14  The ease of using a Large Language Model (LLM)...           14  \n",
       "15  The goal of program synthesis, or code generat...           15  \n",
       "16  Text-to-SQL aims at generating SQL queries for...           16  \n",
       "17  This study examines the performance of open-so...           17  \n",
       "18  Large language models (LLMs) are trained to im...           18  \n",
       "19  Large Language Models (LLMs) contribute signif...           19  \n",
       "20  This research investigates the application of ...           20  \n",
       "21  Large language models (LLMs) have formulated a...           21  \n",
       "22  The advent of large language models (LLMs) has...           22  \n",
       "23  The advancement in generative AI could be boos...           23  \n",
       "24  Large language models (LLMs) such as those emb...           24  \n",
       "25  Large language models (LLMs) encode a vast amo...           25  \n",
       "26  Brain-computer interfaces (BCI) are an importa...           26  \n",
       "27  We propose MM-REACT, a system paradigm that in...           27  \n",
       "28  Recent investigations show that large language...           28  \n",
       "29  We evaluated the capability of a generative pr...           29  \n",
       "30  Knowledge Graphs (KG) provide us with a struct...           30  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>symbex</td>\n",
       "      <td>Find the Python code for specified symbols</td>\n",
       "      <td>0.826333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt-researcher</td>\n",
       "      <td>GPT based autonomous agent that does online co...</td>\n",
       "      <td>0.788222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sweep</td>\n",
       "      <td>Sweep is an AI junior developer</td>\n",
       "      <td>0.776889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>floneum</td>\n",
       "      <td>A graph editor for local AI workflows</td>\n",
       "      <td>0.774904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GPTools</td>\n",
       "      <td>Composable tools for doing useful things with ...</td>\n",
       "      <td>0.769932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dify</td>\n",
       "      <td>One API for plugins and datasets, one interfac...</td>\n",
       "      <td>0.766427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>agenteval</td>\n",
       "      <td>Automated testing and benchmarking for code ge...</td>\n",
       "      <td>0.765689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data-Science-For-Beginners</td>\n",
       "      <td>10 Weeks, 20 Lessons, Data Science for All!</td>\n",
       "      <td>0.762093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chathub</td>\n",
       "      <td>All-in-one chatbot client</td>\n",
       "      <td>0.761229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Calculating Originality of LLM Assisted Source...</td>\n",
       "      <td>The ease of using a Large Language Model (LLM)...</td>\n",
       "      <td>0.754616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gpt-engineer</td>\n",
       "      <td>Specify what you want it to build, the AI asks...</td>\n",
       "      <td>0.748904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama2.c</td>\n",
       "      <td>Inference Llama 2 in one file of pure C</td>\n",
       "      <td>0.739317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>chatgpt-mac</td>\n",
       "      <td>ChatGPT for Mac, living in your menubar.</td>\n",
       "      <td>0.736510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>jsonformer</td>\n",
       "      <td>A Bulletproof Way to Generate Structured JSON ...</td>\n",
       "      <td>0.734520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Open-Source Large Language Models Outperform C...</td>\n",
       "      <td>This study examines the performance of open-so...</td>\n",
       "      <td>0.734234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Math Agents: Computational Infrastructure, Mat...</td>\n",
       "      <td>The advancement in generative AI could be boos...</td>\n",
       "      <td>0.732427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Chit-Chat or Deep Talk: Prompt Engineering for...</td>\n",
       "      <td>This research investigates the application of ...</td>\n",
       "      <td>0.732175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Harnessing LLMs in Curricular Design: Using GP...</td>\n",
       "      <td>We evaluated the capability of a generative pr...</td>\n",
       "      <td>0.728362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Can large language models democratize access t...</td>\n",
       "      <td>Large language models (LLMs) such as those emb...</td>\n",
       "      <td>0.726003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Adapting Transformer Language Models for Predi...</td>\n",
       "      <td>Brain-computer interfaces (BCI) are an importa...</td>\n",
       "      <td>0.725011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Retrieval-augmented GPT-3.5-based Text-to-SQL ...</td>\n",
       "      <td>Text-to-SQL aims at generating SQL queries for...</td>\n",
       "      <td>0.722428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>RLTF: Reinforcement Learning from Unit Test Fe...</td>\n",
       "      <td>The goal of program synthesis, or code generat...</td>\n",
       "      <td>0.722086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>LLM-assisted Knowledge Graph Engineering: Expe...</td>\n",
       "      <td>Knowledge Graphs (KG) provide us with a struct...</td>\n",
       "      <td>0.721514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Can LLMs like GPT-4 outperform traditional AI ...</td>\n",
       "      <td>Recent investigations show that large language...</td>\n",
       "      <td>0.716072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>MM-REACT: Prompting ChatGPT for Multimodal Rea...</td>\n",
       "      <td>We propose MM-REACT, a system paradigm that in...</td>\n",
       "      <td>0.715541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>InvokeAI</td>\n",
       "      <td>InvokeAI is a leading creative engine for Stab...</td>\n",
       "      <td>0.715327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Enabling Intelligent Interactions between an A...</td>\n",
       "      <td>Large language models (LLMs) encode a vast amo...</td>\n",
       "      <td>0.713486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Unleashing the Potential of LLMs for Quantum C...</td>\n",
       "      <td>Large Language Models (LLMs) contribute signif...</td>\n",
       "      <td>0.707834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Secrets of RLHF in Large Language Models Part ...</td>\n",
       "      <td>Large language models (LLMs) have formulated a...</td>\n",
       "      <td>0.707804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Personality Traits in Large Language Models</td>\n",
       "      <td>The advent of large language models (LLMs) has...</td>\n",
       "      <td>0.706053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Do Models Explain Themselves? Counterfactual S...</td>\n",
       "      <td>Large language models (LLMs) are trained to im...</td>\n",
       "      <td>0.704916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "11                                             symbex   \n",
       "5                                      gpt-researcher   \n",
       "3                                               sweep   \n",
       "6                                             floneum   \n",
       "13                                            GPTools   \n",
       "9                                                dify   \n",
       "8                                           agenteval   \n",
       "1                          Data-Science-For-Beginners   \n",
       "2                                             chathub   \n",
       "14  Calculating Originality of LLM Assisted Source...   \n",
       "12                                       gpt-engineer   \n",
       "4                                            llama2.c   \n",
       "7                                         chatgpt-mac   \n",
       "10                                         jsonformer   \n",
       "17  Open-Source Large Language Models Outperform C...   \n",
       "23  Math Agents: Computational Infrastructure, Mat...   \n",
       "20  Chit-Chat or Deep Talk: Prompt Engineering for...   \n",
       "29  Harnessing LLMs in Curricular Design: Using GP...   \n",
       "24  Can large language models democratize access t...   \n",
       "26  Adapting Transformer Language Models for Predi...   \n",
       "16  Retrieval-augmented GPT-3.5-based Text-to-SQL ...   \n",
       "15  RLTF: Reinforcement Learning from Unit Test Fe...   \n",
       "30  LLM-assisted Knowledge Graph Engineering: Expe...   \n",
       "28  Can LLMs like GPT-4 outperform traditional AI ...   \n",
       "27  MM-REACT: Prompting ChatGPT for Multimodal Rea...   \n",
       "0                                            InvokeAI   \n",
       "25  Enabling Intelligent Interactions between an A...   \n",
       "19  Unleashing the Potential of LLMs for Quantum C...   \n",
       "21  Secrets of RLHF in Large Language Models Part ...   \n",
       "22        Personality Traits in Large Language Models   \n",
       "18  Do Models Explain Themselves? Counterfactual S...   \n",
       "\n",
       "                                          description  similarity  \n",
       "11         Find the Python code for specified symbols    0.826333  \n",
       "5   GPT based autonomous agent that does online co...    0.788222  \n",
       "3                     Sweep is an AI junior developer    0.776889  \n",
       "6              A graph editor for local AI workflows     0.774904  \n",
       "13  Composable tools for doing useful things with ...    0.769932  \n",
       "9   One API for plugins and datasets, one interfac...    0.766427  \n",
       "8   Automated testing and benchmarking for code ge...    0.765689  \n",
       "1         10 Weeks, 20 Lessons, Data Science for All!    0.762093  \n",
       "2                           All-in-one chatbot client    0.761229  \n",
       "14  The ease of using a Large Language Model (LLM)...    0.754616  \n",
       "12  Specify what you want it to build, the AI asks...    0.748904  \n",
       "4             Inference Llama 2 in one file of pure C    0.739317  \n",
       "7            ChatGPT for Mac, living in your menubar.    0.736510  \n",
       "10  A Bulletproof Way to Generate Structured JSON ...    0.734520  \n",
       "17  This study examines the performance of open-so...    0.734234  \n",
       "23  The advancement in generative AI could be boos...    0.732427  \n",
       "20  This research investigates the application of ...    0.732175  \n",
       "29  We evaluated the capability of a generative pr...    0.728362  \n",
       "24  Large language models (LLMs) such as those emb...    0.726003  \n",
       "26  Brain-computer interfaces (BCI) are an importa...    0.725011  \n",
       "16  Text-to-SQL aims at generating SQL queries for...    0.722428  \n",
       "15  The goal of program synthesis, or code generat...    0.722086  \n",
       "30  Knowledge Graphs (KG) provide us with a struct...    0.721514  \n",
       "28  Recent investigations show that large language...    0.716072  \n",
       "27  We propose MM-REACT, a system paradigm that in...    0.715541  \n",
       "0   InvokeAI is a leading creative engine for Stab...    0.715327  \n",
       "25  Large language models (LLMs) encode a vast amo...    0.713486  \n",
       "19  Large Language Models (LLMs) contribute signif...    0.707834  \n",
       "21  Large language models (LLMs) have formulated a...    0.707804  \n",
       "22  The advent of large language models (LLMs) has...    0.706053  \n",
       "18  Large language models (LLMs) are trained to im...    0.704916  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def search_knowledgebase(query):\n",
    "    # Get the embedding of the query\n",
    "    query_embedding = get_embeddings(query)\n",
    "    query_embedding = np.array(query_embedding).reshape(1, -1)\n",
    "\n",
    "    # Calculate cosine similarities\n",
    "    similarities = cosine_similarity(query_embedding, embeddings_array)\n",
    "    similarities = similarities.flatten()\n",
    "\n",
    "    # Create a DataFrame for easy manipulation\n",
    "    df = df_metadata.copy()\n",
    "    df['similarity'] = similarities\n",
    "\n",
    "    # Sort by similarity\n",
    "    df_sorted = df.sort_values(by='similarity', ascending=False)\n",
    "\n",
    "    # Get the ranked list of titles and descriptions\n",
    "    results = df_sorted[['title', 'description', 'similarity']]\n",
    "\n",
    "    return results\n",
    "\n",
    "search_knowledgebase(\"Python code search\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write embeddings_array to pickle file\n",
    "import pickle\n",
    "with open('embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings_array, f)\n",
    "# Write df_metadata to csv\n",
    "df_metadata.to_csv(\"metadata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
